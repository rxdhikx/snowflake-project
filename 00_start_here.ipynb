{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24f4735",
   "metadata": {},
   "source": [
    "## SET UP ENVIRONMENT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898c514-831d-4aa7-9697-004994953950",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_set_context"
   },
   "outputs": [],
   "source": [
    "SET MY_USER = CURRENT_USER();\n",
    "\n",
    "SET GITHUB_SECRET_USERNAME = 'username';\n",
    "SET GITHUB_SECRET_PASSWORD = 'personal access token';\n",
    "SET GITHUB_URL_PREFIX = 'https://github.com/username';\n",
    "SET GITHUB_REPO_ORIGIN = 'https://github.com/username/sfguide-data-engineering-with-notebooks.git';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc608c96-0957-47e1-8492-bc8d382925e3",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_account_objects"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the account level objects (ACCOUNTADMIN part)\n",
    "-- ----------------------------------------------------------------------------\n",
    "\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "-- Roles\n",
    "CREATE OR REPLACE ROLE DEMO_ROLE;\n",
    "GRANT ROLE DEMO_ROLE TO ROLE SYSADMIN;\n",
    "GRANT ROLE DEMO_ROLE TO USER IDENTIFIER($MY_USER);\n",
    "\n",
    "GRANT CREATE INTEGRATION ON ACCOUNT TO ROLE DEMO_ROLE;\n",
    "GRANT EXECUTE TASK ON ACCOUNT TO ROLE DEMO_ROLE;\n",
    "GRANT EXECUTE MANAGED TASK ON ACCOUNT TO ROLE DEMO_ROLE;\n",
    "GRANT MONITOR EXECUTION ON ACCOUNT TO ROLE DEMO_ROLE;\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE SNOWFLAKE TO ROLE DEMO_ROLE;\n",
    "\n",
    "-- Databases\n",
    "CREATE OR REPLACE DATABASE DEMO_DB;\n",
    "GRANT OWNERSHIP ON DATABASE DEMO_DB TO ROLE DEMO_ROLE;\n",
    "\n",
    "-- Warehouses\n",
    "CREATE OR REPLACE WAREHOUSE DEMO_WH WAREHOUSE_SIZE = XSMALL, AUTO_SUSPEND = 300, AUTO_RESUME= TRUE;\n",
    "GRANT OWNERSHIP ON WAREHOUSE DEMO_WH TO ROLE DEMO_ROLE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e2ae2c-241b-4d8f-aa99-11a35f9833a4",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_database_objects"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the database level objects\n",
    "-- ----------------------------------------------------------------------------\n",
    "USE ROLE DEMO_ROLE;\n",
    "USE WAREHOUSE DEMO_WH;\n",
    "USE DATABASE DEMO_DB;\n",
    "\n",
    "-- Schemas\n",
    "CREATE OR REPLACE SCHEMA INTEGRATIONS;\n",
    "CREATE OR REPLACE SCHEMA DEV_SCHEMA;\n",
    "CREATE OR REPLACE SCHEMA PROD_SCHEMA;\n",
    "\n",
    "USE SCHEMA INTEGRATIONS;\n",
    "\n",
    "-- External Frostbyte objects\n",
    "CREATE OR REPLACE STAGE FROSTBYTE_RAW_STAGE\n",
    "    URL = 's3://sfquickstarts/data-engineering-with-snowpark-python/'\n",
    ";\n",
    "\n",
    "-- Secrets (schema level)\n",
    "CREATE OR REPLACE SECRET DEMO_GITHUB_SECRET\n",
    "  TYPE = password\n",
    "  USERNAME = $GITHUB_SECRET_USERNAME\n",
    "  PASSWORD = $GITHUB_SECRET_PASSWORD;\n",
    "\n",
    "-- API Integration (account level)\n",
    "-- This depends on the schema level secret!\n",
    "CREATE OR REPLACE API INTEGRATION DEMO_GITHUB_API_INTEGRATION\n",
    "  API_PROVIDER = GIT_HTTPS_API\n",
    "  API_ALLOWED_PREFIXES = ($GITHUB_URL_PREFIX)\n",
    "  ALLOWED_AUTHENTICATION_SECRETS = (DEMO_GITHUB_SECRET)\n",
    "  ENABLED = TRUE;\n",
    "\n",
    "-- Git Repository\n",
    "CREATE OR REPLACE GIT REPOSITORY DEMO_GIT_REPO\n",
    "  API_INTEGRATION = DEMO_GITHUB_API_INTEGRATION\n",
    "  GIT_CREDENTIALS = DEMO_GITHUB_SECRET\n",
    "  ORIGIN = $GITHUB_REPO_ORIGIN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f26add-547e-4d60-8897-d5ad79b3311d",
   "metadata": {
    "language": "sql",
    "name": "sql_step03_event_table"
   },
   "outputs": [],
   "source": [
    "-- ----------------------------------------------------------------------------\n",
    "-- Create the event table\n",
    "-- ----------------------------------------------------------------------------\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\n",
    "GRANT SELECT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n",
    "GRANT INSERT ON EVENT TABLE DEMO_DB.INTEGRATIONS.DEMO_EVENTS TO ROLE DEMO_ROLE;\n",
    "\n",
    "ALTER ACCOUNT SET EVENT_TABLE = DEMO_DB.INTEGRATIONS.DEMO_EVENTS;\n",
    "ALTER DATABASE DEMO_DB SET LOG_LEVEL = INFO;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531119f-76fc-4a2f-a635-a5a7526ac152",
   "metadata": {
    "collapsed": false,
    "name": "md_step04_deploy_dev_notebooks"
   },
   "source": [
    "## Deploy to Dev\n",
    "\n",
    "Finally we will use `EXECUTE IMMEDIATE FROM <file>` along with Jinja templating to deploy the Dev version of our Notebooks. We will directly execute the SQL script `scripts/deploy_notebooks.sql` from our Git repository which has the SQL commands to deploy a Notebook from a Git repo.\n",
    "\n",
    "See [EXECUTE IMMEDIATE FROM](https://docs.snowflake.com/en/sql-reference/sql/execute-immediate-from) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8676d0-7f82-4639-a5e2-29f7f9dca0f5",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "sql_step04_deploy_dev_notebooks"
   },
   "outputs": [],
   "source": [
    "USE ROLE DEMO_ROLE;\n",
    "USE WAREHOUSE DEMO_WH;\n",
    "USE SCHEMA DEMO_DB.INTEGRATIONS;\n",
    "\n",
    "EXECUTE IMMEDIATE FROM @DEMO_GIT_REPO/branches/main/scripts/deploy_notebooks.sql\n",
    "    USING (env => 'DEV', branch => 'dev');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753bb327-95e4-4559-b7c7-f034607196c9",
   "metadata": {
    "collapsed": false,
    "name": "md_step05"
   },
   "source": [
    "## Load Weather\n",
    "\n",
    "But what about data that needs constant updating - like the WEATHER data? We would need to build a pipeline process to constantly update that data to keep it fresh.\n",
    "\n",
    "Perhaps a better way to get this external data would be to source it from a trusted data supplier. Let them manage the data, keeping it accurate and up to date.\n",
    "\n",
    "Enter the Snowflake Data Cloud...\n",
    "\n",
    "Weather Source is a leading provider of global weather and climate data and their OnPoint Product Suite provides businesses with the necessary weather and climate data to quickly generate meaningful and actionable insights for a wide range of use cases across industries. Let's connect to the \"Weather Source LLC: frostbyte\" feed from Weather Source in the Snowflake Data Marketplace by following these steps in Snowsight\n",
    "\n",
    "* In the left navigation bar click on \"Data Products\" and then \"Marketplace\"\n",
    "* Search: \"Weather Source LLC: frostbyte\" (and click on tile in results)\n",
    "* Click the blue \"Get\" button\n",
    "* Under \"Options\", adjust the Database name to read \"FROSTBYTE_WEATHERSOURCE\" (all capital letters)\n",
    "* Grant to \"HOL_ROLE\"\n",
    "\n",
    "That's it... we don't have to do anything from here to keep this data updated. The provider will do that for us and data sharing means we are always seeing whatever they they have published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a850e3-44a4-4829-882e-84724f7e77d7",
   "metadata": {
    "language": "sql",
    "name": "sql_step05_create_share"
   },
   "outputs": [],
   "source": [
    "/*---\n",
    "-- You can also do it via code if you know the account/share details...\n",
    "SET WEATHERSOURCE_ACCT_NAME = '*** PUT ACCOUNT NAME HERE AS PART OF DEMO SETUP ***';\n",
    "SET WEATHERSOURCE_SHARE_NAME = '*** PUT ACCOUNT SHARE HERE AS PART OF DEMO SETUP ***';\n",
    "SET WEATHERSOURCE_SHARE = $WEATHERSOURCE_ACCT_NAME || '.' || $WEATHERSOURCE_SHARE_NAME;\n",
    "\n",
    "CREATE OR REPLACE DATABASE FROSTBYTE_WEATHERSOURCE\n",
    "  FROM SHARE IDENTIFIER($WEATHERSOURCE_SHARE);\n",
    "\n",
    "GRANT IMPORTED PRIVILEGES ON DATABASE FROSTBYTE_WEATHERSOURCE TO ROLE HOL_ROLE;\n",
    "---*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2762d1-fe91-4a7c-b89a-56e1baf0001c",
   "metadata": {
    "language": "sql",
    "name": "sql_step05_test_share"
   },
   "outputs": [],
   "source": [
    "-- Let's look at the data - same 3-part naming convention as any other table\n",
    "SELECT * FROM FROSTBYTE_WEATHERSOURCE.ONPOINT_ID.POSTAL_CODES LIMIT 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d93974-9a75-46c2-876f-95b6e1562f75",
   "metadata": {
    "collapsed": false,
    "name": "md_step06"
   },
   "source": [
    "## Load Excel Files\n",
    "\n",
    " run the `load_excel_files` Notebook. That Notebook will define the pipeline used to load data into the `LOCATION` and `ORDER_DETAIL` tables from the staged Excel files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5587283-a264-444d-b9ec-55b4d926a5c7",
   "metadata": {
    "collapsed": false,
    "name": "md_step07"
   },
   "source": [
    "## Load Daily City Metrics\n",
    "\n",
    "run the `load_daily_city_metrics` Notebook. That Notebook will define the pipeline used to create the `DAILY_CITY_METRICS` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfff6b-067e-4f24-8424-19d0231c0ee1",
   "metadata": {
    "language": "sql",
    "name": "sql_step07_logs"
   },
   "outputs": [],
   "source": [
    "USE ROLE DEMO_ROLE;\n",
    "USE WAREHOUSE DEMO_WH;\n",
    "USE SCHEMA DEMO_DB.INTEGRATIONS;\n",
    "\n",
    "SELECT TOP 100\n",
    "  RECORD['severity_text'] AS SEVERITY,\n",
    "  VALUE AS MESSAGE\n",
    "FROM\n",
    "  DEMO_DB.INTEGRATIONS.DEMO_EVENTS\n",
    "WHERE 1 = 1\n",
    "  AND SCOPE['name'] = 'demo_logger'\n",
    "  AND RECORD_TYPE = 'LOG';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c873a1db-1fbe-4a44-b02a-03e1b2084cb2",
   "metadata": {
    "collapsed": false,
    "name": "md_step08"
   },
   "source": [
    "## Orchestrate Pipelines\n",
    "\n",
    "In this step we will create a DAG (or Directed Acyclic Graph) of Tasks using the new Snowflake Python Management API The Task DAG API builds upon the Python Management API to provide advanced Task management capabilities. \n",
    "\n",
    "This code is also available in the `scripts/deploy_task_dag.py` script which could be used to automate the Task DAG deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac9ad2-2858-4fb7-b3a2-6394db5b0b4c",
   "metadata": {
    "language": "python",
    "name": "py_step08_imports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from snowflake.core import Root\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "session.use_role(\"DEMO_ROLE\")\n",
    "session.use_warehouse(\"DEMO_WH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bbc8e-c525-4cd0-b147-a832a9060c47",
   "metadata": {
    "language": "python",
    "name": "py_step08_set_env"
   },
   "outputs": [],
   "source": [
    "database_name = \"DEMO_DB\"\n",
    "schema_name = \"DEV_SCHEMA\"\n",
    "#schema_name = \"PROD_SCHEMA\"\n",
    "env = 'PROD' if schema_name == 'PROD_SCHEMA' else 'DEV'\n",
    "\n",
    "session.use_schema(f\"{database_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c030976-5888-4d9f-a329-3248b25abd78",
   "metadata": {
    "language": "python",
    "name": "py_step08_create_dag"
   },
   "outputs": [],
   "source": [
    "from snowflake.core.task.dagv1 import DAGOperation, DAG, DAGTask\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the tasks using the DAG API\n",
    "warehouse_name = \"DEMO_WH\"\n",
    "dag_name = \"DEMO_DAG\"\n",
    "\n",
    "api_root = Root(session)\n",
    "schema = api_root.databases[database_name].schemas[schema_name]\n",
    "dag_op = DAGOperation(schema)\n",
    "\n",
    "# Define the DAG\n",
    "with DAG(dag_name, schedule=timedelta(days=1), warehouse=warehouse_name) as dag:\n",
    "    dag_task1 = DAGTask(\"LOAD_EXCEL_FILES_TASK\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_06_load_excel_files\"()''', warehouse=warehouse_name)\n",
    "    dag_task2 = DAGTask(\"LOAD_DAILY_CITY_METRICS\", definition=f'''EXECUTE NOTEBOOK \"{database_name}\".\"{schema_name}\".\"{env}_07_load_daily_city_metrics\"()''', warehouse=warehouse_name)\n",
    "\n",
    "    # Define the dependencies between the tasks\n",
    "    dag_task1 >> dag_task2 # dag_task1 is a predecessor of dag_task2\n",
    "\n",
    "# Create the DAG in Snowflake\n",
    "dag_op.deploy(dag, mode=\"orreplace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560c909-5523-4a12-ad21-0b9044bdaff6",
   "metadata": {
    "language": "python",
    "name": "py_step08_run_dag"
   },
   "outputs": [],
   "source": [
    "dagiter = dag_op.iter_dags(like='demo_dag%')\n",
    "for dag_name in dagiter:\n",
    "    print(dag_name)\n",
    "\n",
    "#dag_op.run(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad46ffc-1137-43dc-add8-7fc02914bbaa",
   "metadata": {
    "collapsed": false,
    "name": "md_step09"
   },
   "source": [
    "## Deploy to Production\n",
    "\n",
    "Steps\n",
    "1. Make a small change to a notebook and commit it to the dev branch\n",
    "1. Go into GitHub and create a PR and Merge to main branch\n",
    "1. Review GitHub Actions workflow definition and run results\n",
    "1. See new \"PROD_\" versions of the Notebooks\n",
    "1. Deploy the production version of the task DAG\n",
    "1. Run production version of the task DAG and see new tables created!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba497c01-0988-4c07-af66-79ee2918cffa",
   "metadata": {
    "collapsed": false,
    "name": "md_step10"
   },
   "source": [
    "## Teardown\n",
    "\n",
    "Finally, we will tear down our demo environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ca116-4585-4668-bb72-cf74b0e7b587",
   "metadata": {
    "language": "sql",
    "name": "sql_step10"
   },
   "outputs": [],
   "source": [
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "DROP API INTEGRATION DEMO_GITHUB_API_INTEGRATION;\n",
    "DROP DATABASE DEMO_DB;\n",
    "DROP WAREHOUSE DEMO_WH;\n",
    "DROP ROLE DEMO_ROLE;\n",
    "\n",
    "-- Drop the weather share\n",
    "DROP DATABASE FROSTBYTE_WEATHERSOURCE;\n",
    "\n",
    "-- Remove the \"dev\" branch in your repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
